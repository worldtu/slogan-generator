{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('valid.csv')\n",
    "test_set = pd.read_csv('test-curated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "train_set, validation_set = train_test_split(train_set, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing - data cleaning and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESC_COL = 'desc'\n",
    "SLOGAN_COL = 'output'\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,?!']\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_set, validation_set, test_set]:\n",
    "    df[DESC_COL + '_cleaned'] = df[DESC_COL].apply(clean_text)\n",
    "    df[SLOGAN_COL + '_cleaned'] = df[SLOGAN_COL].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8079fa7107734f9ca711558aaf8593bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c83eaaae6b4248b56292ddfcbfde9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc6621602334aedb624935788d94972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb7621ce4fd4af8a526735d98ea2159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_name = 'facebook/bart-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize_text_with_transformers(text_series, tokenizer_model, max_len=128):\n",
    "    encoded_inputs = tokenizer_model(\n",
    "        text_series.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    return encoded_inputs['input_ids']\n",
    "\n",
    "for df in [train_set, validation_set, test_set]:\n",
    "    df[DESC_COL + '_tokenized'] = tokenize_text_with_transformers(df[DESC_COL + '_cleaned'], tokenizer)\n",
    "    df[SLOGAN_COL + '_tokenized'] = tokenize_text_with_transformers(df[SLOGAN_COL + '_cleaned'], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "desc_col = DESC_COL + '_tokenized'\n",
    "slogan_col = SLOGAN_COL + '_tokenized'\n",
    "# Build torch dataset\n",
    "class SloganDataset(Dataset):\n",
    "    def __init__(self, dataframe, desc_tokenizer, slogan_tokenizer, \n",
    "                 desc_col, slogan_col, \n",
    "                 max_desc_len, max_slogan_len,\n",
    "                 bos_token_id, eos_token_id, pad_token_id):\n",
    "        self.dataframe = dataframe\n",
    "        self.desc_tokenizer = desc_tokenizer\n",
    "        self.slogan_tokenizer = slogan_tokenizer\n",
    "        self.desc_col = desc_col\n",
    "        self.slogan_col = slogan_col\n",
    "        self.max_desc_len = max_desc_len\n",
    "        self.max_slogan_len = max_slogan_len\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        desc_text = str(row[self.desc_col])\n",
    "        slogan_text = str(row[self.slogan_col])\n",
    "\n",
    "        desc_encoded = self.desc_tokenizer.encode(desc_text, \n",
    "        add_special_tokens=False, \n",
    "        max_length=self.max_desc_len, \n",
    "        padding='max_length', \n",
    "        truncation=True)\n",
    "        encoder_input_ids = torch.tensor(desc_encoded, dtype=torch.long)\n",
    "\n",
    "\n",
    "        slogan_encoded = self.slogan_tokenizer.encode(slogan_text, add_special_tokens=False,\n",
    "        max_length=self.max_slogan_len - 2,\n",
    "        truncation=True) \n",
    "        slogan_ids = slogan_encoded\n",
    "\n",
    "        decoder_input_ids = [self.bos_token_id] + slogan_ids\n",
    "        decoder_target_ids = slogan_ids + [self.eos_token_id]\n",
    "\n",
    "        return {\n",
    "            'encoder_input_ids': torch.tensor(encoder_input_ids, dtype=torch.long),\n",
    "            'decoder_input_ids': torch.tensor(decoder_input_ids, dtype=torch.long),\n",
    "            'decoder_target_ids': torch.tensor(decoder_target_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        encoder_inputs = pad_sequence([item['encoder_input_ids'] for item in batch], \n",
    "                                      batch_first=True, padding_value=self.pad_token_id)\n",
    "        decoder_inputs = pad_sequence([item['decoder_input_ids'] for item in batch], \n",
    "                                      batch_first=True, padding_value=self.pad_token_id)\n",
    "\n",
    "        padding_for_targets = self.pad_token_id if self.pad_token_id != -100 else -100\n",
    "\n",
    "        decoder_targets = pad_sequence([item['decoder_target_ids'] for item in batch], \n",
    "                                       batch_first=True, padding_value=padding_for_targets)\n",
    "        \n",
    "        return {\n",
    "            'encoder_input_ids': encoder_inputs,\n",
    "            'decoder_input_ids': decoder_inputs,\n",
    "            'decoder_target_ids': decoder_targets\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask_pt(seq, pad_token_id):\n",
    "    return seq == pad_token_id\n",
    "\n",
    "def create_src_padding_mask_pt(seq, pad_token_id):\n",
    "    return seq == pad_token_id\n",
    "\n",
    "def create_tgt_padding_mask_pt(seq, pad_token_id):\n",
    "    return seq == pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, d_model, nhead, num_encoder_layers,\n",
    "                 num_decoder_layers, dim_feedforward, max_seq_length, dropout=0.1, batch_first=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.source_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=self.batch_first\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = (torch.triu(torch.ones(sz, sz, device=device)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask # shape (sz, sz)\n",
    "\n",
    "    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None, memory_key_padding_mask=None):\n",
    "        src_emb = self.source_embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.target_embedding(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "\n",
    "        tgt_seq_len = tgt.size(1) if self.batch_first else tgt.size(0)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt_seq_len, src.device)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src_emb,\n",
    "            tgt_emb,\n",
    "            src_mask=None,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        return self.fc_out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedulePT:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.n_steps += 1\n",
    "        lr = self._get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "    def _get_lr(self):\n",
    "        current_step = float(self.n_steps)\n",
    "        if self.d_model == 0: return 0.0\n",
    "        factor = self.d_model ** -0.5\n",
    "        arg1 = current_step ** -0.5\n",
    "        if self.warmup_steps > 0:\n",
    "            arg2 = current_step * (self.warmup_steps ** -1.5)\n",
    "            return factor * min(arg1, arg2)\n",
    "        else:\n",
    "            return factor * arg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_TRAIN = 20\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "MAX_DESC_LEN = 128\n",
    "BATCH_FIRST = True\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id\n",
    "BOS_TOKEN_ID = tokenizer.bos_token_id\n",
    "EOS_TOKEN_ID = tokenizer.eos_token_id\n",
    "INPUT_VOCAB_SIZE = tokenizer.vocab_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        slogan_tokenizer,\n",
    "        dataset: Dataset,\n",
    "        val_dataset: Dataset = None,\n",
    "        batch_size: int = 32,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 0.0001,\n",
    "        warmup_steps: int = 0,\n",
    "        d_model: int = 128,\n",
    "        device: str = \"cpu\",\n",
    "        pad_token_id: int = 0\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.slogan_tokenizer = slogan_tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            collate_fn=getattr(dataset, 'collate_fn', None)\n",
    "        )\n",
    "        if self.val_dataset:\n",
    "            self.val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                collate_fn=getattr(val_dataset, 'collate_fn', None)\n",
    "            )\n",
    "\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98), eps=1e-9)\n",
    "        self.lr_scheduler = None \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_token_id if self.pad_token_id != -100 else -100)\n",
    "\n",
    "    def train_epoch(self, epoch_num, total_epochs):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        current_lr = self.optim.param_groups[0]['lr']\n",
    "        # current_lr = 0\n",
    "        progress_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch_num+1}/{total_epochs} [T]\")\n",
    "        \n",
    "        for batch_data in progress_bar:\n",
    "            src = batch_data['encoder_input_ids'].to(self.device)\n",
    "            tgt_input = batch_data['decoder_input_ids'].to(self.device)\n",
    "            tgt_real = batch_data['decoder_target_ids'].to(self.device)\n",
    "\n",
    "            src_padding_mask = create_padding_mask_pt(src, self.pad_token_id)\n",
    "            tgt_padding_mask = create_padding_mask_pt(tgt_input, self.pad_token_id)\n",
    "            \n",
    "            self.optim.zero_grad()\n",
    "\n",
    "            logits = self.model(src, tgt_input, \n",
    "                                src_padding_mask=src_padding_mask, \n",
    "                                tgt_padding_mask=tgt_padding_mask,\n",
    "                                memory_key_padding_mask=src_padding_mask)\n",
    "\n",
    "            B, T, V = logits.shape\n",
    "            loss = self.criterion(logits.reshape(B*T, V), tgt_real.reshape(B*T))\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            # current_lr = self.lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{current_lr:.7f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        return avg_loss, current_lr\n",
    "\n",
    "    def evaluate_epoch(self, epoch_num, total_epochs):\n",
    "        if not self.val_loader:\n",
    "            return None, None, None\n",
    "            \n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        all_predictions_text = [] \n",
    "        all_references_text = []\n",
    "        \n",
    "        progress_bar = tqdm(self.val_loader, desc=f\"Epoch {epoch_num+1}/{total_epochs} [V]\")\n",
    "\n",
    "        printed_samples_this_epoch = False\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(progress_bar):\n",
    "                src = batch_data['encoder_input_ids'].to(self.device)\n",
    "                tgt_input = batch_data['decoder_input_ids'].to(self.device)\n",
    "                tgt_real = batch_data['decoder_target_ids'].to(self.device)\n",
    "\n",
    "                src_padding_mask = create_padding_mask_pt(src, self.pad_token_id)\n",
    "                tgt_padding_mask = create_padding_mask_pt(tgt_input, self.pad_token_id)\n",
    "\n",
    "                logits = self.model(src, tgt_input,\n",
    "                                    src_padding_mask=src_padding_mask,\n",
    "                                    tgt_padding_mask=tgt_padding_mask,\n",
    "                                    memory_key_padding_mask=src_padding_mask)\n",
    "                \n",
    "                B, T, V = logits.shape\n",
    "                loss = self.criterion(logits.reshape(B*T, V), tgt_real.reshape(B*T))\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "                predicted_ids_batch = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                current_batch_preds_for_metric = []\n",
    "                current_batch_refs_for_metric = []\n",
    "                \n",
    "                for i in range(B):\n",
    "                    raw_pred_ids = predicted_ids_batch[i].tolist()\n",
    "                    \n",
    "                    pred_ids_before_eos_truncation = raw_pred_ids[:]\n",
    "                    \n",
    "                    processed_pred_ids_for_decode = raw_pred_ids[:]\n",
    "                    try:\n",
    "                        eos_idx = raw_pred_ids.index(self.slogan_tokenizer.eos_token_id)\n",
    "                        processed_pred_ids_for_decode = raw_pred_ids[:eos_idx]\n",
    "                    except (ValueError, AttributeError): \n",
    "                        # ValueError: EOS token not found in the list\n",
    "                        # AttributeError: if slogan_tokenizer or eos_token_id is missing\n",
    "                        pass\n",
    "                    final_pred_text = self.slogan_tokenizer.decode(processed_pred_ids_for_decode, skip_special_tokens=True)\n",
    "                    current_batch_preds_for_metric.append(final_pred_text)\n",
    "\n",
    "                    raw_ref_ids = tgt_real[i].tolist()\n",
    "                    \n",
    "                    filtered_ref_ids_for_decode = [\n",
    "                        token_id for token_id in raw_ref_ids \n",
    "                        if token_id != self.pad_token_id and \\\n",
    "                           (not hasattr(self.slogan_tokenizer, 'eos_token_id') or token_id != self.slogan_tokenizer.eos_token_id)\n",
    "                    ]\n",
    "                    \n",
    "                    final_ref_text = self.slogan_tokenizer.decode(filtered_ref_ids_for_decode, skip_special_tokens=True)\n",
    "                    current_batch_refs_for_metric.append(final_ref_text)\n",
    "\n",
    "                    if batch_idx == 0 and i < 3 and not printed_samples_this_epoch:\n",
    "                        print(f\"\\n--- Epoch {epoch_num+1} Validation Sample {i} ---\")\n",
    "                        print(f\"Pad Token ID: {self.pad_token_id}, EOS Token ID: {getattr(self.slogan_tokenizer, 'eos_token_id', 'N/A')}\")\n",
    "                        \n",
    "                        print(f\"  Raw Predicted IDs: {raw_pred_ids}\")\n",
    "                        # 解码原始预测 (包含特殊token)\n",
    "                        decoded_raw_pred_no_skip = self.slogan_tokenizer.decode(raw_pred_ids, skip_special_tokens=False)\n",
    "                        print(f\"  Decoded Raw Predicted (skip_special_tokens=False): '{decoded_raw_pred_no_skip}'\")\n",
    "                        # 解码原始预测 (跳过特殊token)\n",
    "                        decoded_raw_pred_skip = self.slogan_tokenizer.decode(raw_pred_ids, skip_special_tokens=True)\n",
    "                        print(f\"  Decoded Raw Predicted (skip_special_tokens=True): '{decoded_raw_pred_skip}'\")\n",
    "                        \n",
    "                        print(f\"  Processed Predicted IDs (for final decode, after EOS cut): {processed_pred_ids_for_decode}\")\n",
    "                        print(f\"  FINAL Decoded Prediction (for metric): '{final_pred_text}'\")\n",
    "                        \n",
    "                        print(f\"  Raw Reference IDs: {raw_ref_ids}\")\n",
    "                        decoded_raw_ref_no_skip = self.slogan_tokenizer.decode(raw_ref_ids, skip_special_tokens=False)\n",
    "                        print(f\"  Decoded Raw Reference (skip_special_tokens=False): '{decoded_raw_ref_no_skip}'\")\n",
    "\n",
    "                        print(f\"  Processed Reference IDs (for final decode, after PAD/EOS filter): {filtered_ref_ids_for_decode}\")\n",
    "                        print(f\"  FINAL Decoded Reference (for metric): '{final_ref_text}'\")\n",
    "                        print(\"--- End Sample ---\")\n",
    "                \n",
    "                if batch_idx == 0:\n",
    "                    printed_samples_this_epoch = True\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "\n",
    "        # ROUGE\n",
    "        rouge_results = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "        if all_predictions_text and all_references_text:\n",
    "            for pred_text, ref_text in zip(all_predictions_text, all_references_text):\n",
    "                if not pred_text.strip():\n",
    "                    rouge_results['rouge1'].append(0.0)\n",
    "                    rouge_results['rouge2'].append(0.0)\n",
    "                    rouge_results['rougeL'].append(0.0)\n",
    "                else:\n",
    "                    actual_scores = self.rouge_eval_scorer.score(ref_text, pred_text)\n",
    "                    rouge_results['rouge1'].append(actual_scores['rouge1'].fmeasure)\n",
    "                    rouge_results['rouge2'].append(actual_scores['rouge2'].fmeasure)\n",
    "                    rouge_results['rougeL'].append(actual_scores['rougeL'].fmeasure)\n",
    "            \n",
    "            avg_rouge1 = np.mean(rouge_results['rouge1']) if rouge_results['rouge1'] else 0\n",
    "            avg_rouge2 = np.mean(rouge_results['rouge2']) if rouge_results['rouge2'] else 0\n",
    "            avg_rougeL = np.mean(rouge_results['rougeL']) if rouge_results['rougeL'] else 0\n",
    "            avg_rouge_scores = {'rouge1': avg_rouge1, 'rouge2': avg_rouge2, 'rougeL': avg_rougeL}\n",
    "        else:\n",
    "            avg_rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "\n",
    "        # BLEU\n",
    "        bleu_scores_list = []\n",
    "        if all_predictions_text and all_references_text:\n",
    "            for pred_text, ref_text in zip(all_predictions_text, all_references_text):\n",
    "                ref_tokens = [self.slogan_tokenizer.tokenize(ref_text)]\n",
    "                pred_tokens = self.slogan_tokenizer.tokenize(pred_text)\n",
    "                \n",
    "                if not pred_tokens:\n",
    "                    bleu_scores_list.append(0.0)\n",
    "                    continue\n",
    "\n",
    "                score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=self.bleu_smoothing_function.method1)\n",
    "                bleu_scores_list.append(score)\n",
    "            \n",
    "            avg_bleu_score = np.mean(bleu_scores_list) if bleu_scores_list else 0\n",
    "        else:\n",
    "            avg_bleu_score = 0\n",
    "\n",
    "        return avg_loss, avg_rouge_scores, avg_bleu_score\n",
    "\n",
    "    def train(self, epochs: int = 5, model_save_path: str = \"my_transformer_model.pt\"):\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "            avg_train_loss, current_lr = self.train_epoch(epoch, epochs)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Avg Train Loss: {avg_train_loss:.4f}, LR: {current_lr:.7f}\")\n",
    "\n",
    "            avg_val_loss, avg_rouge_scores, avg_bleu_score = self.evaluate_epoch(epoch, epochs)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Avg Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"  Avg ROUGE-1: {avg_rouge_scores['rouge1']:.4f}, ROUGE-2: {avg_rouge_scores['rouge2']:.4f}, ROUGE-L: {avg_rouge_scores['rougeL']:.4f}\")\n",
    "            print(f\"  Avg BLEU: {avg_bleu_score:.4f}\")\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                model_dir = os.path.dirname(model_save_path)\n",
    "                if model_dir and not os.path.exists(model_dir):\n",
    "                    os.makedirs(model_dir, exist_ok=True)\n",
    "                torch.save(self.model.state_dict(), model_save_path)\n",
    "                print(f\"Model improved and saved to {model_save_path}\")\n",
    "        \n",
    "        print(\"\\nTraining finished.\")\n",
    "        print(f\"Final model weights saved to {model_save_path} (if not overwritten by better validation scores).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slogan_dataset = SloganDataset(\n",
    "    train_set, tokenizer, tokenizer,\n",
    "    DESC_COL, SLOGAN_COL,\n",
    "    128, 128,\n",
    "    BOS_TOKEN_ID, EOS_TOKEN_ID, PAD_TOKEN_ID\n",
    ")\n",
    "val_slogan_dataset = SloganDataset(\n",
    "    validation_set, tokenizer, tokenizer,\n",
    "    DESC_COL, SLOGAN_COL,\n",
    "    128, 128,\n",
    "    BOS_TOKEN_ID, EOS_TOKEN_ID, PAD_TOKEN_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    input_vocab_size=INPUT_VOCAB_SIZE,\n",
    "    target_vocab_size=INPUT_VOCAB_SIZE,\n",
    "    d_model=d_model,\n",
    "    nhead=num_heads,\n",
    "    num_encoder_layers=num_layers,\n",
    "    num_decoder_layers=num_layers,\n",
    "    dim_feedforward=dff,\n",
    "    max_seq_length=MAX_DESC_LEN,\n",
    "    dropout=dropout_rate,\n",
    "    batch_first=BATCH_FIRST\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    slogan_tokenizer=tokenizer,\n",
    "    dataset=train_slogan_dataset,\n",
    "    val_dataset=val_slogan_dataset,\n",
    "    device=device,\n",
    "    pad_token_id=PAD_TOKEN_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [T]:   0%|          | 0/134 [00:00<?, ?it/s]/tmp/ipykernel_1353/2477025848.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'encoder_input_ids': torch.tensor(encoder_input_ids, dtype=torch.long),\n",
      "/venv/main/lib/python3.10/site-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Epoch 1/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.29it/s, loss=7.4568, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Avg Train Loss: 8.3128, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.55it/s, loss=7.4612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): ',,,</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): ',,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [6, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): ',,,'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 1 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): ',,,,</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): ',,,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [6, 6, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): ',,,,'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 1 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): ',,,,</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): ',,,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [6, 6, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): ',,,,'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.54it/s, loss=6.9103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Avg Validation Loss: 7.6196\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 2/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.21it/s, loss=7.1615, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Avg Train Loss: 7.1505, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 23.19it/s, loss=7.4978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 2 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 8, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The and and</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The and and'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 2 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 8, 8, 8, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The and and and</s> and</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 8, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The and and and'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 2 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 8, 8, 8, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The and and and</s></s></s></s> and</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 8, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The and and and'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 23.68it/s, loss=6.5718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Avg Validation Loss: 7.4763\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 3/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.26it/s, loss=7.2337, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Avg Train Loss: 6.8845, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.01it/s, loss=7.4379]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 3 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 154, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theing</s></s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 154]\n",
      "  FINAL Decoded Prediction (for metric): 'Theing'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 3 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 154, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theing and</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theing and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 154, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'Theing and'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 3 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 154, 154, 8, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theinging and</s></s></s></s> the</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theinging and the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 154, 154, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'Theinging and'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 23.96it/s, loss=6.5124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Avg Validation Loss: 7.4395\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 4/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.20it/s, loss=6.6483, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Avg Train Loss: 6.7152, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.51it/s, loss=7.1361]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 4 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 154, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theing,</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theing,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 154, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'Theing,'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 4 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 8, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- and,</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- and,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 8, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The- and,'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 4 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 12, 5, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-- the</s></s></s></s> the</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-- the the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 12, 5]\n",
      "  FINAL Decoded Prediction (for metric): 'The-- the'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.91it/s, loss=6.4500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Avg Validation Loss: 7.3926\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 5/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.17it/s, loss=6.4800, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Avg Train Loss: 6.5765, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.54it/s, loss=7.1501]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 5 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing</s>,,,,,,,,,,'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing,,,,,,,,,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 5 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing,, the,,,,,,,,'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing,, the,,,,,,,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing,, the,,,,,,,,'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 5 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 6801, 12, 12, 6, 6, 2, 2, 5, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theury--,,</s></s> the</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theury--,, the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 6801, 12, 12, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'Theury--,,'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.31it/s, loss=6.4841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Avg Validation Loss: 7.4163\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 6/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.22it/s, loss=6.1954, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Avg Train Loss: 6.4219, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.56it/s, loss=7.1634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 6 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 7438, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The Design and</s> and and and and and and and and and and'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The Design and and and and and and and and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 7438, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The Design and'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 6 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 359, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing & and the and and and and and and and and'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing & and the and and and and and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 359, 8, 5, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing & and the and and and and and and and and'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 6 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 2088, 7438, 3779, 7438, 3779, 1820, 2, 3779, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theive Design IT Design IT Services</s> IT</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theive Design IT Design IT Services IT'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 2088, 7438, 3779, 7438, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Theive Design IT Design IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.64it/s, loss=6.3723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Avg Validation Loss: 7.4177\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 7/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.10it/s, loss=6.1002, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Avg Train Loss: 6.2367, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.79it/s, loss=7.0265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 7 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- and and and and and and and and and and and and'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- and and and and and and and and and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The- and and and and and and and and and and and and'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 7 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 6, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing, and the,,,,,,,,'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing, and the,,,,,,,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 6, 8, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing, and the,,,,,,,,'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 7 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 6801, 5, 5, 6, 6, 1820, 2, 3779, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theury the the,, Services</s> IT</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theury the the,, Services IT'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 6801, 5, 5, 6, 6, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Theury the the,, Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.26it/s, loss=6.1945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Avg Validation Loss: 7.3077\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 8/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.06it/s, loss=5.6980, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Avg Train Loss: 6.0344, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 23.93it/s, loss=7.2047]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 8 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 9, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The of of</s> for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The of of for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 9]\n",
      "  FINAL Decoded Prediction (for metric): 'The of of'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 8 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 359, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing &</s> the</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing & the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 8 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9020, 6282, 9020, 4628, 359, 1820, 2, 3779, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The Marketing Digital Marketingaged & Services</s> IT</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The Marketing Digital Marketingaged & Services IT'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9020, 6282, 9020, 4628, 359, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'The Marketing Digital Marketingaged & Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 23.27it/s, loss=6.0241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Avg Validation Loss: 7.3398\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 9/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [T]: 100%|██████████| 134/134 [00:13<00:00,  9.95it/s, loss=6.0985, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Avg Train Loss: 5.8349, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 23.48it/s, loss=7.0982]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 9 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- and and and and and and and and and and and and'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- and and and and and and and and and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The- and and and and and and and and and and and and'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 9 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 154, 154, 359, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theinging &, the,,,,,,,,'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theinging &, the,,,,,,,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 154, 154, 359, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'Theinging &, the,,,,,,,,'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 9 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 6801, 26491, 9020, 4628, 8, 1820, 2, 9020, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theuryruit Marketingaged and Services</s> Marketing</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theuryruit Marketingaged and Services Marketing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 6801, 26491, 9020, 4628, 8, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Theuryruit Marketingaged and Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 23.52it/s, loss=5.9617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Avg Validation Loss: 7.2246\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 10/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [T]: 100%|██████████| 134/134 [00:13<00:00,  9.97it/s, loss=6.0666, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Avg Train Loss: 5.6419, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 21.76it/s, loss=7.1254]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 10 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 8, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- and</s> and and and and and and and and and and'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- and and and and and and and and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The- and'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 10 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 359, 8, 5, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing & and the</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing & and the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 359, 8, 5]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing & and the'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 10 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 3779, 3779, 12, 4628, 6, 1820, 2, 3779, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The IT IT-aged, Services</s> IT</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The IT IT-aged, Services IT'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 3779, 3779, 12, 4628, 6, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'The IT IT-aged, Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 21.96it/s, loss=5.8662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Avg Validation Loss: 7.2596\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 11/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.10it/s, loss=6.0046, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Avg Train Loss: 5.4600, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 23.91it/s, loss=6.9480]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 11 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The's of and and and and and and and and and and and'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The's of and and and and and and and and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The's of and and and and and and and and and and and'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 11 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 154, 359, 8, 5, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The ofing & and the</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The ofing & and the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 154, 359, 8, 5]\n",
      "  FINAL Decoded Prediction (for metric): 'The ofing & and the'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 11 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 9020, 26491, 9020, 4628, 1820, 1820, 2, 9020, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Web Marketingruit Marketingaged Services Services</s> Marketing</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Web Marketingruit Marketingaged Services Services Marketing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 9020, 26491, 9020, 4628, 1820, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Web Marketingruit Marketingaged Services Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.34it/s, loss=5.8221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Avg Validation Loss: 7.1700\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 12/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.23it/s, loss=5.2968, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Avg Train Loss: 5.2687, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.33it/s, loss=6.8699]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 12 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 9, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The of of</s> and and and and and and and and and and'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The of of and and and and and and and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 9]\n",
      "  FINAL Decoded Prediction (for metric): 'The of of'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 12 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 154, 2, 8, 2, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The ofing</s> and</s> of of of of of of of of'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The ofing and of of of of of of of of'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 154]\n",
      "  FINAL Decoded Prediction (for metric): 'The ofing'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 12 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 9020, 9020, 9020, 4628, 6, 1820, 2, 9020, 2, 2, 1437, 1437, 1437]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Web Marketing Marketing Marketingaged, Services</s> Marketing</s></s>   '\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Web Marketing Marketing Marketingaged, Services Marketing   '\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 9020, 9020, 9020, 4628, 6, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Web Marketing Marketing Marketingaged, Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.58it/s, loss=5.6820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Avg Validation Loss: 7.0999\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 13/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.09it/s, loss=5.1404, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Avg Train Loss: 5.1007, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.30it/s, loss=6.9435]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 13 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 9, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The of of for for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The of of for for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 9, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The of of for for for for for for for for for for for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 13 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 154, 154, 359, 359, 5, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theinging & & the of of of of of of of of'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theinging & & the of of of of of of of of'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 154, 154, 359, 359, 5, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "  FINAL Decoded Prediction (for metric): 'Theinging & & the of of of of of of of of'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 13 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 3779, 26491, 9020, 4628, 3779, 1820, 2, 9020, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Web ITruit Marketingaged IT Services</s> Marketing</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Web ITruit Marketingaged IT Services Marketing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 3779, 26491, 9020, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Web ITruit Marketingaged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.18it/s, loss=5.6445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Avg Validation Loss: 7.1637\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 14/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.24it/s, loss=4.6863, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Avg Train Loss: 4.9229, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.24it/s, loss=7.0215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 14 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 9, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The of of</s> and and and and and and and and and and'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The of of and and and and and and and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 9]\n",
      "  FINAL Decoded Prediction (for metric): 'The of of'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 14 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 6074, 154, 359, 8, 5, 6, 6, 6, 6, 8, 8, 8, 6]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theininging & and the,,,, and and and,'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theininging & and the,,,, and and and,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 6074, 154, 359, 8, 5, 6, 6, 6, 6, 8, 8, 8, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'Theininging & and the,,,, and and and,'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 14 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 3779, 6282, 9020, 4628, 3779, 1820, 2, 9020, 2, 2, 1437, 1437, 1437]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Web IT Digital Marketingaged IT Services</s> Marketing</s></s>   '\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Web IT Digital Marketingaged IT Services Marketing   '\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 3779, 6282, 9020, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Web IT Digital Marketingaged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.84it/s, loss=5.6201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Avg Validation Loss: 7.2318\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 15/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.22it/s, loss=5.0163, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Avg Train Loss: 4.7560, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.52it/s, loss=6.8774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 15 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 9, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The of of</s> for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The of of for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 9]\n",
      "  FINAL Decoded Prediction (for metric): 'The of of'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 15 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 23862, 154, 359, 359, 359, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theesticing & & & for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theesticing & & & for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 23862, 154, 359, 359, 359, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'Theesticing & & & for for for for for for for for'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 15 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 1851, 6282, 9020, 4628, 3779, 1820, 2, 9020, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Webain Digital Marketingaged IT Services</s> Marketing</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Webain Digital Marketingaged IT Services Marketing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 1851, 6282, 9020, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Webain Digital Marketingaged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.64it/s, loss=5.4927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Avg Validation Loss: 7.0786\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 16/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.15it/s, loss=4.4009, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Avg Train Loss: 4.5832, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 23.83it/s, loss=6.9984]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 16 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 18, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The's's for for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The's's for for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 18, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The's's for for for for for for for for for for for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 16 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 1295, 154, 359, 359, 359, 359, 359, 359, 359, 359, 359, 359, 359]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theatinging & & & & & & & & & & &'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theatinging & & & & & & & & & & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 1295, 154, 359, 359, 359, 359, 359, 359, 359, 359, 359, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'Theatinging & & & & & & & & & & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 16 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 1851, 2010, 9020, 4628, 6, 1820, 2, 9020, 1820, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Webain Security Marketingaged, Services</s> Marketing Services</s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Webain Security Marketingaged, Services Marketing Services'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 1851, 2010, 9020, 4628, 6, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Webain Security Marketingaged, Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.33it/s, loss=5.4751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Avg Validation Loss: 7.1721\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 17/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.21it/s, loss=4.3553, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Avg Train Loss: 4.4092, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.55it/s, loss=6.9412]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 17 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 9, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The of of for for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The of of for for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 9, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The of of for for for for for for for for for for for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 17 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 575, 154, 359, 359, 666, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The careing & & India,,,,,,,,'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The careing & & India,,,,,,,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 575, 154, 359, 359, 666, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The careing & & India,,,,,,,,'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 17 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 1851, 6282, 9020, 4628, 359, 1820, 2, 9020, 1820, 2, 1437, 1437, 1437]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Webain Digital Marketingaged & Services</s> Marketing Services</s>   '\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Webain Digital Marketingaged & Services Marketing Services   '\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 1851, 6282, 9020, 4628, 359, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Webain Digital Marketingaged & Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.75it/s, loss=5.3909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Avg Validation Loss: 7.1234\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 18/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.21it/s, loss=4.4887, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Avg Train Loss: 4.2360, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 23.92it/s, loss=6.9860]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 18 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 9, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The of of</s> and and and and and and and and and and'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The of of and and and and and and and and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 9]\n",
      "  FINAL Decoded Prediction (for metric): 'The of of'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 18 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 575, 154, 359, 359, 666, 359, 359, 359, 359, 359, 359, 359, 359]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The careing & & India & & & & & & & &'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The careing & & India & & & & & & & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 575, 154, 359, 359, 666, 359, 359, 359, 359, 359, 359, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The careing & & India & & & & & & & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 18 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 1851, 6282, 9020, 4628, 3779, 1820, 2, 9020, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Webain Digital Marketingaged IT Services</s> Marketing</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Webain Digital Marketingaged IT Services Marketing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 1851, 6282, 9020, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Webain Digital Marketingaged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 24.12it/s, loss=5.4248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Avg Validation Loss: 7.1844\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 19/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.24it/s, loss=4.2159, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Avg Train Loss: 4.0925, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.89it/s, loss=7.0648]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 19 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 9, 2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The of of</s> for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The of of for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 9]\n",
      "  FINAL Decoded Prediction (for metric): 'The of of'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 19 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 575, 154, 359, 359, 359, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The careing & & &</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The careing & & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 575, 154, 359, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The careing & & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 19 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 1851, 9020, 359, 4628, 359, 1820, 2, 9020, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Webain Marketing &aged & Services</s> Marketing</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Webain Marketing &aged & Services Marketing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 1851, 9020, 359, 4628, 359, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Webain Marketing &aged & Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [V]: 100%|██████████| 34/34 [00:01<00:00, 25.10it/s, loss=5.4284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Avg Validation Loss: 7.2471\n",
      "  Avg ROUGE-1: 0.0000, ROUGE-2: 0.0000, ROUGE-L: 0.0000\n",
      "  Avg BLEU: 0.0000\n",
      "\n",
      "--- Epoch 20/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [T]: 100%|██████████| 134/134 [00:13<00:00, 10.20it/s, loss=3.5403, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Avg Train Loss: 3.9221, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [V]:   9%|▉         | 3/34 [00:00<00:01, 24.57it/s, loss=6.9569]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 20 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 9, 9, 2, 8, 8, 8, 8, 8, 8, 8, 8, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The of of</s> and and and and and and and and for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The of of and and and and and and and and for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 9, 9]\n",
      "  FINAL Decoded Prediction (for metric): 'The of of'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 20 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 23862, 154, 359, 359, 359, 359, 359, 359, 359, 359, 359, 359, 359]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theesticing & & & & & & & & & & &'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theesticing & & & & & & & & & & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 23862, 154, 359, 359, 359, 359, 359, 359, 359, 359, 359, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'Theesticing & & & & & & & & & & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 20 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 1851, 3779, 9020, 4628, 3779, 1820, 2, 3779, 1820, 2, 1437, 1437, 1437]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Webain IT Marketingaged IT Services</s> IT Services</s>   '\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Webain IT Marketingaged IT Services IT Services   '\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 1851, 3779, 9020, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Webain IT Marketingaged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [V]:  79%|███████▉  | 27/34 [00:01<00:00, 24.73it/s, loss=7.6075]"
     ]
    }
   ],
   "source": [
    "trainer.train(epochs=EPOCHS_TRAIN, model_save_path=\"final_ipynb_transformer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1440/2307428834.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (source_embedding): Embedding(50265, 512)\n",
       "  (target_embedding): Embedding(50265, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"final_ipynb_transformer.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape        Output Shape       Param #\n",
       "=======================================================================================================================\n",
       "Transformer (Transformer)                                         [64, 128]          [64, 127, 50265]   --\n",
       "├─Embedding (source_embedding)                                    [64, 128]          [64, 128, 512]     25,735,680\n",
       "├─Embedding (target_embedding)                                    [64, 127]          [64, 127, 512]     25,735,680\n",
       "├─PositionalEncoding (pos_encoder)                                [64, 128, 512]     [64, 128, 512]     --\n",
       "│    └─Dropout (dropout)                                          [64, 128, 512]     [64, 128, 512]     --\n",
       "├─PositionalEncoding (pos_encoder)                                [64, 127, 512]     [64, 127, 512]     --\n",
       "│    └─Dropout (dropout)                                          [64, 127, 512]     [64, 127, 512]     --\n",
       "├─Transformer (transformer)                                       [64, 128, 512]     [64, 127, 512]     --\n",
       "│    └─TransformerEncoder (encoder)                               [64, 128, 512]     [64, 128, 512]     --\n",
       "│    │    └─ModuleList (layers)                                   --                 --                 18,914,304\n",
       "│    │    └─LayerNorm (norm)                                      [64, 128, 512]     [64, 128, 512]     1,024\n",
       "│    └─TransformerDecoder (decoder)                               [64, 127, 512]     [64, 127, 512]     --\n",
       "│    │    └─ModuleList (layers)                                   --                 --                 25,224,192\n",
       "│    │    └─LayerNorm (norm)                                      [64, 127, 512]     [64, 127, 512]     1,024\n",
       "├─Linear (fc_out)                                                 [64, 127, 512]     [64, 127, 50265]   25,785,945\n",
       "=======================================================================================================================\n",
       "Total params: 121,397,849\n",
       "Trainable params: 121,397,849\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.56\n",
       "=======================================================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 6409.44\n",
       "Params size (MB): 409.95\n",
       "Estimated Total Size (MB): 6819.52\n",
       "======================================================================================================================="
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.randint(0, 50265, (64, 128)).to(device)\n",
    "tgt = torch.randint(0, 50265, (64, 127)).to(device)\n",
    "\n",
    "summary(model, \n",
    "        input_data=(src, tgt),\n",
    "        depth=3,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        row_settings=[\"var_names\"],\n",
    "        col_width=18\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layer (name)                        | Output Shape      | # Params     |\n",
    "|------------------------------------|-------------------|--------------|\n",
    "| Embedding (source_embedding)       | [64, 128, 512]    | 25,735,680   |\n",
    "| Embedding (target_embedding)       | [64, 127, 512]    | 25,735,680   |\n",
    "| PositionalEncoding (src)           | [64, 128, 512]    | 0            |\n",
    "| PositionalEncoding (tgt)           | [64, 127, 512]    | 0            |\n",
    "| TransformerEncoder (6 layers)      | [64, 128, 512]    | 18,914,304   |\n",
    "| TransformerDecoder (6 layers)      | [64, 127, 512]    | 25,224,192   |\n",
    "| Linear (fc_out)                    | [64, 127, 50265]  | 25,785,945   |\n",
    "| **Total**                          | —                 | **121,397,849** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 121,397,849\n",
      "Trainable parameters: 121,397,849\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_set.iloc[3][\"desc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(example, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_DESC_LEN)\n",
    "source_tensor = encoded_input['input_ids'].to(device)\n",
    "source_attention_mask = encoded_input['attention_mask'].to(device)\n",
    "\n",
    "src_key_padding_mask = (source_attention_mask == 0)\n",
    "\n",
    "target_tensor_input = torch.tensor([[BOS_TOKEN_ID]], dtype=torch.long, device=device)\n",
    "generated_ids = [BOS_TOKEN_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for _ in range(MAX_DESC_LEN):\n",
    "        tgt_mask = model._generate_square_subsequent_mask(target_tensor_input.size(1), device)\n",
    "        output_logits = model(src=source_tensor, \n",
    "                              tgt=target_tensor_input, \n",
    "                              src_padding_mask=src_key_padding_mask, \n",
    "                              memory_key_padding_mask=src_key_padding_mask\n",
    "                            #   tgt_mask=tgt_mask\n",
    "                              )\n",
    "        next_token_logits = output_logits[:, -1, :]\n",
    "        predicted_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "        target_tensor_input = torch.cat((target_tensor_input, predicted_token_id.unsqueeze(1)), dim=1)\n",
    "        generated_ids.append(predicted_token_id.item())\n",
    "\n",
    "        if predicted_token_id.item() == EOS_TOKEN_ID:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "slogan = tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Best Best's Most Trusted in the UK\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slogan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
