{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('valid.csv')\n",
    "test_set = pd.read_csv('test-curated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "train_set, validation_set = train_test_split(train_set, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing - data cleaning and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESC_COL = 'desc'\n",
    "SLOGAN_COL = 'output'\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,?!']\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_set, validation_set, test_set]:\n",
    "    df[DESC_COL + '_cleaned'] = df[DESC_COL].apply(clean_text)\n",
    "    df[SLOGAN_COL + '_cleaned'] = df[SLOGAN_COL].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750dabdc7e054c32a1c339e8b04da34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5945b4a5ae59450097e44dbfa59dbf64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9707a25bf84542e6b038f83a27951db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82806cc7212b49e0a6a1e98d32db1bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_name = 'facebook/bart-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "def tokenize_text_with_transformers(text_series, tokenizer_model, max_len=128):\n",
    "    encoded_inputs = tokenizer_model(\n",
    "        text_series.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    return encoded_inputs['input_ids']\n",
    "\n",
    "for df in [train_set, validation_set, test_set]:\n",
    "    df[DESC_COL + '_tokenized'] = tokenize_text_with_transformers(df[DESC_COL + '_cleaned'], tokenizer)\n",
    "    df[SLOGAN_COL + '_tokenized'] = tokenize_text_with_transformers(df[SLOGAN_COL + '_cleaned'], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "desc_col = DESC_COL + '_tokenized'\n",
    "slogan_col = SLOGAN_COL + '_tokenized'\n",
    "# Build torch dataset\n",
    "class SloganDataset(Dataset):\n",
    "    def __init__(self, dataframe, desc_tokenizer, slogan_tokenizer, \n",
    "                 desc_col, slogan_col, \n",
    "                 max_desc_len, max_slogan_len,\n",
    "                 bos_token_id, eos_token_id, pad_token_id):\n",
    "        self.dataframe = dataframe\n",
    "        self.desc_tokenizer = desc_tokenizer\n",
    "        self.slogan_tokenizer = slogan_tokenizer\n",
    "        self.desc_col = desc_col\n",
    "        self.slogan_col = slogan_col\n",
    "        self.max_desc_len = max_desc_len\n",
    "        self.max_slogan_len = max_slogan_len\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        desc_text = str(row[self.desc_col])\n",
    "        slogan_text = str(row[self.slogan_col])\n",
    "\n",
    "        desc_encoded = self.desc_tokenizer.encode(desc_text, \n",
    "        add_special_tokens=False, \n",
    "        max_length=self.max_desc_len, \n",
    "        padding='max_length', \n",
    "        truncation=True)\n",
    "        encoder_input_ids = torch.tensor(desc_encoded, dtype=torch.long)\n",
    "\n",
    "\n",
    "        slogan_encoded = self.slogan_tokenizer.encode(slogan_text, add_special_tokens=False,\n",
    "        max_length=self.max_slogan_len - 2,\n",
    "        truncation=True) \n",
    "        slogan_ids = slogan_encoded\n",
    "\n",
    "        decoder_input_ids = [self.bos_token_id] + slogan_ids\n",
    "        decoder_target_ids = slogan_ids + [self.eos_token_id]\n",
    "\n",
    "        return {\n",
    "            'encoder_input_ids': torch.tensor(encoder_input_ids, dtype=torch.long),\n",
    "            'decoder_input_ids': torch.tensor(decoder_input_ids, dtype=torch.long),\n",
    "            'decoder_target_ids': torch.tensor(decoder_target_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        encoder_inputs = pad_sequence([item['encoder_input_ids'] for item in batch], \n",
    "                                      batch_first=True, padding_value=self.pad_token_id)\n",
    "        decoder_inputs = pad_sequence([item['decoder_input_ids'] for item in batch], \n",
    "                                      batch_first=True, padding_value=self.pad_token_id)\n",
    "\n",
    "        padding_for_targets = self.pad_token_id if self.pad_token_id != -100 else -100\n",
    "\n",
    "        decoder_targets = pad_sequence([item['decoder_target_ids'] for item in batch], \n",
    "                                       batch_first=True, padding_value=padding_for_targets)\n",
    "        \n",
    "        return {\n",
    "            'encoder_input_ids': encoder_inputs,\n",
    "            'decoder_input_ids': decoder_inputs,\n",
    "            'decoder_target_ids': decoder_targets\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask_pt(seq, pad_token_id):\n",
    "    return seq == pad_token_id\n",
    "\n",
    "def create_src_padding_mask_pt(seq, pad_token_id):\n",
    "    return seq == pad_token_id\n",
    "\n",
    "def create_tgt_padding_mask_pt(seq, pad_token_id):\n",
    "    return seq == pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, d_model, nhead, num_encoder_layers,\n",
    "                 num_decoder_layers, dim_feedforward, max_seq_length, dropout=0.1, batch_first=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.source_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=self.batch_first\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz, device):\n",
    "        mask = (torch.triu(torch.ones(sz, sz, device=device)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask # shape (sz, sz)\n",
    "\n",
    "    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None, memory_key_padding_mask=None):\n",
    "        src_emb = self.source_embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.target_embedding(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "\n",
    "        tgt_seq_len = tgt.size(1) if self.batch_first else tgt.size(0)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt_seq_len, src.device)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src_emb,\n",
    "            tgt_emb,\n",
    "            src_mask=None,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        return self.fc_out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedulePT:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.n_steps += 1\n",
    "        lr = self._get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return lr\n",
    "\n",
    "    def _get_lr(self):\n",
    "        current_step = float(self.n_steps)\n",
    "        if self.d_model == 0: return 0.0\n",
    "        factor = self.d_model ** -0.5\n",
    "        arg1 = current_step ** -0.5\n",
    "        if self.warmup_steps > 0:\n",
    "            arg2 = current_step * (self.warmup_steps ** -1.5)\n",
    "            return factor * min(arg1, arg2)\n",
    "        else:\n",
    "            return factor * arg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_TRAIN = 20\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "MAX_DESC_LEN = 128\n",
    "BATCH_FIRST = True\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id\n",
    "BOS_TOKEN_ID = tokenizer.bos_token_id\n",
    "EOS_TOKEN_ID = tokenizer.eos_token_id\n",
    "INPUT_VOCAB_SIZE = tokenizer.vocab_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        slogan_tokenizer,\n",
    "        dataset: Dataset,\n",
    "        val_dataset: Dataset = None,\n",
    "        batch_size: int = 32,\n",
    "        lr: float = 1e-4,\n",
    "        weight_decay: float = 0.0001,\n",
    "        warmup_steps: int = 0,\n",
    "        d_model: int = 128,\n",
    "        device: str = \"cpu\",\n",
    "        pad_token_id: int = 0\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.slogan_tokenizer = slogan_tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.rouge_eval_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.bleu_smoothing_function = SmoothingFunction()\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            collate_fn=getattr(dataset, 'collate_fn', None)\n",
    "        )\n",
    "        if self.val_dataset:\n",
    "            self.val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                collate_fn=getattr(val_dataset, 'collate_fn', None)\n",
    "            )\n",
    "\n",
    "        self.optim = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98), eps=1e-9)\n",
    "        self.lr_scheduler = None \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_token_id if self.pad_token_id != -100 else -100)\n",
    "\n",
    "    def train_epoch(self, epoch_num, total_epochs):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        current_lr = self.optim.param_groups[0]['lr']\n",
    "        # current_lr = 0\n",
    "        progress_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch_num+1}/{total_epochs} [T]\")\n",
    "        \n",
    "        for batch_data in progress_bar:\n",
    "            src = batch_data['encoder_input_ids'].to(self.device)\n",
    "            tgt_input = batch_data['decoder_input_ids'].to(self.device)\n",
    "            tgt_real = batch_data['decoder_target_ids'].to(self.device)\n",
    "\n",
    "            src_padding_mask = create_padding_mask_pt(src, self.pad_token_id)\n",
    "            tgt_padding_mask = create_padding_mask_pt(tgt_input, self.pad_token_id)\n",
    "            \n",
    "            self.optim.zero_grad()\n",
    "\n",
    "            logits = self.model(src, tgt_input, \n",
    "                                src_padding_mask=src_padding_mask, \n",
    "                                tgt_padding_mask=tgt_padding_mask,\n",
    "                                memory_key_padding_mask=src_padding_mask)\n",
    "\n",
    "            B, T, V = logits.shape\n",
    "            loss = self.criterion(logits.reshape(B*T, V), tgt_real.reshape(B*T))\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            # current_lr = self.lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{current_lr:.7f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        return avg_loss, current_lr\n",
    "\n",
    "    def evaluate_epoch(self, epoch_num, total_epochs):\n",
    "        if not self.val_loader:\n",
    "            return None, None, None\n",
    "            \n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        all_predictions_text = [] \n",
    "        all_references_text = []\n",
    "        \n",
    "        progress_bar = tqdm(self.val_loader, desc=f\"Epoch {epoch_num+1}/{total_epochs} [V]\")\n",
    "\n",
    "        printed_samples_this_epoch = False\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(progress_bar):\n",
    "                src = batch_data['encoder_input_ids'].to(self.device)\n",
    "                tgt_input = batch_data['decoder_input_ids'].to(self.device)\n",
    "                tgt_real = batch_data['decoder_target_ids'].to(self.device)\n",
    "\n",
    "                src_padding_mask = create_padding_mask_pt(src, self.pad_token_id)\n",
    "                tgt_padding_mask = create_padding_mask_pt(tgt_input, self.pad_token_id)\n",
    "\n",
    "                logits = self.model(src, tgt_input,\n",
    "                                    src_padding_mask=src_padding_mask,\n",
    "                                    tgt_padding_mask=tgt_padding_mask,\n",
    "                                    memory_key_padding_mask=src_padding_mask)\n",
    "                \n",
    "                B, T, V = logits.shape\n",
    "                loss = self.criterion(logits.reshape(B*T, V), tgt_real.reshape(B*T))\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "                predicted_ids_batch = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                current_batch_preds_for_metric = []\n",
    "                current_batch_refs_for_metric = []\n",
    "                \n",
    "                for i in range(B):\n",
    "                    raw_pred_ids = predicted_ids_batch[i].tolist()\n",
    "                    \n",
    "                    pred_ids_before_eos_truncation = raw_pred_ids[:]\n",
    "                    \n",
    "                    processed_pred_ids_for_decode = raw_pred_ids[:]\n",
    "                    try:\n",
    "                        eos_idx = raw_pred_ids.index(self.slogan_tokenizer.eos_token_id)\n",
    "                        processed_pred_ids_for_decode = raw_pred_ids[:eos_idx]\n",
    "                    except (ValueError, AttributeError): \n",
    "                        # ValueError: EOS token not found in the list\n",
    "                        # AttributeError: if slogan_tokenizer or eos_token_id is missing\n",
    "                        pass\n",
    "                    final_pred_text = self.slogan_tokenizer.decode(processed_pred_ids_for_decode, skip_special_tokens=True)\n",
    "                    current_batch_preds_for_metric.append(final_pred_text)\n",
    "\n",
    "                    raw_ref_ids = tgt_real[i].tolist()\n",
    "                    \n",
    "                    filtered_ref_ids_for_decode = [\n",
    "                        token_id for token_id in raw_ref_ids \n",
    "                        if token_id != self.pad_token_id and \\\n",
    "                           (not hasattr(self.slogan_tokenizer, 'eos_token_id') or token_id != self.slogan_tokenizer.eos_token_id)\n",
    "                    ]\n",
    "                    \n",
    "                    final_ref_text = self.slogan_tokenizer.decode(filtered_ref_ids_for_decode, skip_special_tokens=True)\n",
    "                    current_batch_refs_for_metric.append(final_ref_text)\n",
    "\n",
    "                    if batch_idx == 0 and i < 3 and not printed_samples_this_epoch:\n",
    "                        print(f\"\\n--- Epoch {epoch_num+1} Validation Sample {i} ---\")\n",
    "                        print(f\"Pad Token ID: {self.pad_token_id}, EOS Token ID: {getattr(self.slogan_tokenizer, 'eos_token_id', 'N/A')}\")\n",
    "                        \n",
    "                        print(f\"  Raw Predicted IDs: {raw_pred_ids}\")\n",
    "                        \n",
    "                        decoded_raw_pred_no_skip = self.slogan_tokenizer.decode(raw_pred_ids, skip_special_tokens=False)\n",
    "                        print(f\"  Decoded Raw Predicted (skip_special_tokens=False): '{decoded_raw_pred_no_skip}'\")\n",
    "                        \n",
    "                        decoded_raw_pred_skip = self.slogan_tokenizer.decode(raw_pred_ids, skip_special_tokens=True)\n",
    "                        print(f\"  Decoded Raw Predicted (skip_special_tokens=True): '{decoded_raw_pred_skip}'\")\n",
    "                        \n",
    "                        print(f\"  Processed Predicted IDs (for final decode, after EOS cut): {processed_pred_ids_for_decode}\")\n",
    "                        print(f\"  FINAL Decoded Prediction (for metric): '{final_pred_text}'\")\n",
    "                        \n",
    "                        print(f\"  Raw Reference IDs: {raw_ref_ids}\")\n",
    "                        decoded_raw_ref_no_skip = self.slogan_tokenizer.decode(raw_ref_ids, skip_special_tokens=False)\n",
    "                        print(f\"  Decoded Raw Reference (skip_special_tokens=False): '{decoded_raw_ref_no_skip}'\")\n",
    "\n",
    "                        print(f\"  Processed Reference IDs (for final decode, after PAD/EOS filter): {filtered_ref_ids_for_decode}\")\n",
    "                        print(f\"  FINAL Decoded Reference (for metric): '{final_ref_text}'\")\n",
    "                        print(\"--- End Sample ---\")\n",
    "                \n",
    "                if batch_idx == 0:\n",
    "                    printed_samples_this_epoch = True\n",
    "\n",
    "                all_predictions_text.extend(current_batch_preds_for_metric)\n",
    "                all_references_text.extend(current_batch_refs_for_metric)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "\n",
    "        # ROUGE\n",
    "        rouge_results = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "        if all_predictions_text and all_references_text:\n",
    "            for pred_text, ref_text in zip(all_predictions_text, all_references_text):\n",
    "                if not pred_text.strip():\n",
    "                    rouge_results['rouge1'].append(0.0)\n",
    "                    rouge_results['rouge2'].append(0.0)\n",
    "                    rouge_results['rougeL'].append(0.0)\n",
    "                else:\n",
    "                    actual_scores = self.rouge_eval_scorer.score(ref_text, pred_text)\n",
    "                    rouge_results['rouge1'].append(actual_scores['rouge1'].fmeasure)\n",
    "                    rouge_results['rouge2'].append(actual_scores['rouge2'].fmeasure)\n",
    "                    rouge_results['rougeL'].append(actual_scores['rougeL'].fmeasure)\n",
    "            \n",
    "            avg_rouge1 = np.mean(rouge_results['rouge1']) if rouge_results['rouge1'] else 0\n",
    "            avg_rouge2 = np.mean(rouge_results['rouge2']) if rouge_results['rouge2'] else 0\n",
    "            avg_rougeL = np.mean(rouge_results['rougeL']) if rouge_results['rougeL'] else 0\n",
    "            avg_rouge_scores = {'rouge1': avg_rouge1, 'rouge2': avg_rouge2, 'rougeL': avg_rougeL}\n",
    "        else:\n",
    "            avg_rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "\n",
    "        # BLEU\n",
    "        bleu_scores_list = []\n",
    "        if all_predictions_text and all_references_text:\n",
    "            for pred_text, ref_text in zip(all_predictions_text, all_references_text):\n",
    "                ref_tokens = [self.slogan_tokenizer.tokenize(ref_text)]\n",
    "                pred_tokens = self.slogan_tokenizer.tokenize(pred_text)\n",
    "                \n",
    "                if not pred_tokens:\n",
    "                    bleu_scores_list.append(0.0)\n",
    "                    continue\n",
    "\n",
    "                score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=self.bleu_smoothing_function.method1)\n",
    "                bleu_scores_list.append(score)\n",
    "            \n",
    "            avg_bleu_score = np.mean(bleu_scores_list) if bleu_scores_list else 0\n",
    "        else:\n",
    "            avg_bleu_score = 0\n",
    "\n",
    "        return avg_loss, avg_rouge_scores, avg_bleu_score\n",
    "\n",
    "    def train(self, epochs: int = 5, model_save_path: str = \"my_transformer_model.pt\"):\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "            avg_train_loss, current_lr = self.train_epoch(epoch, epochs)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Avg Train Loss: {avg_train_loss:.4f}, LR: {current_lr:.7f}\")\n",
    "\n",
    "            avg_val_loss, avg_rouge_scores, avg_bleu_score = self.evaluate_epoch(epoch, epochs)\n",
    "\n",
    "            avg_rouge_scores_pct = {k: v * 100 for k, v in avg_rouge_scores.items()}\n",
    "            avg_bleu_score_pct   = avg_bleu_score * 100\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Avg Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"  Avg ROUGE-1: {avg_rouge_scores_pct['rouge1']:.4f}, ROUGE-2: {avg_rouge_scores_pct['rouge2']:.4f}, ROUGE-L: {avg_rouge_scores_pct['rougeL']:.4f}\")\n",
    "            print(f\"  Avg BLEU: {avg_bleu_score_pct:.4f}\")\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                model_dir = os.path.dirname(model_save_path)\n",
    "                if model_dir and not os.path.exists(model_dir):\n",
    "                    os.makedirs(model_dir, exist_ok=True)\n",
    "                torch.save(self.model.state_dict(), model_save_path)\n",
    "                print(f\"Model improved and saved to {model_save_path}\")\n",
    "        \n",
    "        print(\"\\nTraining finished.\")\n",
    "        print(f\"Final model weights saved to {model_save_path} (if not overwritten by better validation scores).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slogan_dataset = SloganDataset(\n",
    "    train_set, tokenizer, tokenizer,\n",
    "    DESC_COL, SLOGAN_COL,\n",
    "    128, 128,\n",
    "    BOS_TOKEN_ID, EOS_TOKEN_ID, PAD_TOKEN_ID\n",
    ")\n",
    "val_slogan_dataset = SloganDataset(\n",
    "    validation_set, tokenizer, tokenizer,\n",
    "    DESC_COL, SLOGAN_COL,\n",
    "    128, 128,\n",
    "    BOS_TOKEN_ID, EOS_TOKEN_ID, PAD_TOKEN_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    input_vocab_size=INPUT_VOCAB_SIZE,\n",
    "    target_vocab_size=INPUT_VOCAB_SIZE,\n",
    "    d_model=d_model,\n",
    "    nhead=num_heads,\n",
    "    num_encoder_layers=num_layers,\n",
    "    num_decoder_layers=num_layers,\n",
    "    dim_feedforward=dff,\n",
    "    max_seq_length=MAX_DESC_LEN,\n",
    "    dropout=dropout_rate,\n",
    "    batch_first=BATCH_FIRST\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    slogan_tokenizer=tokenizer,\n",
    "    dataset=train_slogan_dataset,\n",
    "    val_dataset=val_slogan_dataset,\n",
    "    device=device,\n",
    "    pad_token_id=PAD_TOKEN_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [T]:   0%|          | 0/134 [00:00<?, ?it/s]/tmp/ipykernel_1372/2477025848.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'encoder_input_ids': torch.tensor(encoder_input_ids, dtype=torch.long),\n",
      "Epoch 1/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 12.03it/s, loss=7.3349, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Avg Train Loss: 8.3161, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.21it/s, loss=7.3272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [8, 8, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): ' and and and</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): ' and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [8, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): ' and and and'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 1 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [8, 8, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): ' and and and</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): ' and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [8, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): ' and and and'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 1 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [8, 8, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): ' and and and</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): ' and and and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [8, 8, 8]\n",
      "  FINAL Decoded Prediction (for metric): ' and and and'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 37.96it/s, loss=6.9007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Avg Validation Loss: 7.6138\n",
      "  Avg ROUGE-1: 4.2712, ROUGE-2: 0.0000, ROUGE-L: 4.2712\n",
      "  Avg BLEU: 0.3703\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 2/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.95it/s, loss=7.2602, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Avg Train Loss: 7.1499, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.24it/s, loss=7.1081]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 2 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The,</s></s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The,'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 2 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The and</s></s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The and'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 2 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 6, 2, 2, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The,</s></s></s></s></s></s> and</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The, and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The,'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.15it/s, loss=6.6715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Avg Validation Loss: 7.4885\n",
      "  Avg ROUGE-1: 3.4408, ROUGE-2: 0.0000, ROUGE-L: 3.4068\n",
      "  Avg BLEU: 0.2254\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 3/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.94it/s, loss=6.1586, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Avg Train Loss: 6.8800, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.42it/s, loss=7.0495]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 3 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 154, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theing,</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theing,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 154, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'Theing,'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 3 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- and</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The- and'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 3 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 154, 154, 8, 2, 8, 2, 2, 9020, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theinging and</s> and</s></s> Marketing</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theinging and and Marketing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 154, 154, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'Theinging and'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.82it/s, loss=6.4899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Avg Validation Loss: 7.4818\n",
      "  Avg ROUGE-1: 4.0171, ROUGE-2: 0.0267, ROUGE-L: 3.9831\n",
      "  Avg BLEU: 0.5122\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 4/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 12.00it/s, loss=6.2307, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Avg Train Loss: 6.7119, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.21it/s, loss=6.9806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 4 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-,,,,,,,,,,,,'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-,,,,,,,,,,,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The-,,,,,,,,,,,,'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 4 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-,,</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The-,,'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 4 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 12, 6, 6, 5, 2, 2, 5, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The--,, the</s></s> the</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The--,, the the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 12, 6, 6, 5]\n",
      "  FINAL Decoded Prediction (for metric): 'The--,, the'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.61it/s, loss=6.5225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Avg Validation Loss: 7.4245\n",
      "  Avg ROUGE-1: 2.4187, ROUGE-2: 0.0000, ROUGE-L: 2.4017\n",
      "  Avg BLEU: 0.4804\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 5/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.98it/s, loss=6.6955, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Avg Train Loss: 6.5658, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.67it/s, loss=6.9072]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 5 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-,,</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The-,,'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 5 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 7073, 8, 6, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theux and, and</s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theux and, and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 7073, 8, 6, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'Theux and, and'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 5 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 5, 8, 8, 5, 2, 2, 5, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- the and and the</s></s> the</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- the and and the the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 5, 8, 8, 5]\n",
      "  FINAL Decoded Prediction (for metric): 'The- the and and the'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.78it/s, loss=6.3886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Avg Validation Loss: 7.4110\n",
      "  Avg ROUGE-1: 4.4001, ROUGE-2: 0.1029, ROUGE-L: 4.3768\n",
      "  Avg BLEU: 0.7840\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 6/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.96it/s, loss=6.7732, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Avg Train Loss: 6.4040, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 37.31it/s, loss=6.8269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 6 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 359, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The's & &</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The's & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The's & &'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 6 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 34044, 154, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theporateing &</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theporateing &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 34044, 154, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'Theporateing &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 6 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 5, 8, 359, 5, 2, 2, 5, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- the and & the</s></s> the</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- the and & the the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 5, 8, 359, 5]\n",
      "  FINAL Decoded Prediction (for metric): 'The- the and & the'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 37.78it/s, loss=6.1550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Avg Validation Loss: 7.3140\n",
      "  Avg ROUGE-1: 4.8130, ROUGE-2: 0.6967, ROUGE-L: 4.7753\n",
      "  Avg BLEU: 1.1912\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 7/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.98it/s, loss=6.3719, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Avg Train Loss: 6.2163, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 37.74it/s, loss=6.7392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 7 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 13, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The's for and</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The's for and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 13, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The's for and'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 7 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing,,</s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing,,'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 6, 6]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing,,'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 7 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 5, 154, 4, 5, 2, 2, 5, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The's theing. the</s></s> the</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The's theing. the the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 5, 154, 4, 5]\n",
      "  FINAL Decoded Prediction (for metric): 'The's theing. the'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.37it/s, loss=6.0238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Avg Validation Loss: 7.2553\n",
      "  Avg ROUGE-1: 6.1808, ROUGE-2: 0.7801, ROUGE-L: 6.0827\n",
      "  Avg BLEU: 1.3788\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 8/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.94it/s, loss=6.3682, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Avg Train Loss: 6.0253, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.40it/s, loss=6.6759]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 8 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 359, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The's & &</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The's & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The's & &'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 8 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 359, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing & &</s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 8 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 8, 8, 8, 3779, 1820, 2, 5, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- and and and IT Services</s> the</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- and and and IT Services the'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 8, 8, 8, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'The- and and and IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.20it/s, loss=5.9460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Avg Validation Loss: 7.2225\n",
      "  Avg ROUGE-1: 6.3472, ROUGE-2: 0.8921, ROUGE-L: 6.2862\n",
      "  Avg BLEU: 1.6140\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 9/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.97it/s, loss=5.7503, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Avg Train Loss: 5.8235, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.48it/s, loss=6.7549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 9 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- for for for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- for for for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The- for for for for for for for for for for for for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 9 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing &</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 9 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 11143, 154, 154, 3779, 1820, 2, 11143, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- Softwareinging IT Services</s> Software</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- Softwareinging IT Services Software'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 11143, 154, 154, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'The- Softwareinging IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.84it/s, loss=5.8446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Avg Validation Loss: 7.3763\n",
      "  Avg ROUGE-1: 7.8294, ROUGE-2: 1.1613, ROUGE-L: 7.6135\n",
      "  Avg BLEU: 1.8743\n",
      "\n",
      "--- Epoch 10/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.95it/s, loss=5.3504, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Avg Train Loss: 5.6264, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.62it/s, loss=6.6133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 10 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 9, 8, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The's of and for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The's of and for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 9, 8, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The's of and for for for for for for for for for for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 10 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 359, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing & and</s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing & and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 359, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing & and'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 10 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 8, 8, 8, 3779, 1820, 2, 2090, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- and and and IT Services</s> Business</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- and and and IT Services Business'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 8, 8, 8, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'The- and and and IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.32it/s, loss=5.6708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Avg Validation Loss: 7.2604\n",
      "  Avg ROUGE-1: 8.7848, ROUGE-2: 1.2648, ROUGE-L: 8.5784\n",
      "  Avg BLEU: 2.1682\n",
      "\n",
      "--- Epoch 11/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.95it/s, loss=5.0948, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Avg Train Loss: 5.4421, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 37.87it/s, loss=6.6209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 11 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 5, 7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The the to</s></s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The the to'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 5, 7]\n",
      "  FINAL Decoded Prediction (for metric): 'The the to'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 11 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing &</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 11 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 11143, 4, 4, 3779, 1820, 2, 9020, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- Software.. IT Services</s> Marketing</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- Software.. IT Services Marketing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 11143, 4, 4, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'The- Software.. IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.33it/s, loss=5.5501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Avg Validation Loss: 7.2854\n",
      "  Avg ROUGE-1: 8.5160, ROUGE-2: 1.5708, ROUGE-L: 8.3887\n",
      "  Avg BLEU: 2.1896\n",
      "\n",
      "--- Epoch 12/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.85it/s, loss=5.5003, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Avg Train Loss: 5.2566, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.04it/s, loss=6.4271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 12 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 7003, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The'soles for</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The'soles for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 7003, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The'soles for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 12 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 1260, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing Company &</s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing Company &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 1260, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing Company &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 12 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 7438, 11143, 154, 4628, 7438, 1820, 2, 2717, 1820, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The Design Softwareingaged Design Services</s> Development Services</s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The Design Softwareingaged Design Services Development Services'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 7438, 11143, 154, 4628, 7438, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'The Design Softwareingaged Design Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.27it/s, loss=5.5113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Avg Validation Loss: 7.0856\n",
      "  Avg ROUGE-1: 8.3987, ROUGE-2: 1.3110, ROUGE-L: 8.1548\n",
      "  Avg BLEU: 2.2597\n",
      "Model improved and saved to final_ipynb_transformer.pt\n",
      "\n",
      "--- Epoch 13/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.89it/s, loss=5.2900, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Avg Train Loss: 5.0847, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.19it/s, loss=6.4974]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 13 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 636, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The'sic for for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The'sic for for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 636, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The'sic for for for for for for for for for for for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 13 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [250, 18, 154, 359, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'A'sing & &</s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'A'sing & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [250, 18, 154, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'A'sing & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 13 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [250, 18, 11143, 4, 4, 3779, 1820, 2, 11143, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'A's Software.. IT Services</s> Software</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'A's Software.. IT Services Software'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [250, 18, 11143, 4, 4, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'A's Software.. IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.25it/s, loss=5.4329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Avg Validation Loss: 7.1164\n",
      "  Avg ROUGE-1: 9.4921, ROUGE-2: 1.8919, ROUGE-L: 9.1724\n",
      "  Avg BLEU: 2.5145\n",
      "\n",
      "--- Epoch 14/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.92it/s, loss=5.1789, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Avg Train Loss: 4.9086, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.48it/s, loss=6.4964]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 14 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 18, 13, 359, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The's for & for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The's for & for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 18, 13, 359, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The's for & for for for for for for for for for for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 14 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 359, 359, 359, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing & & &</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing & & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 359, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing & & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 14 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 11143, 4, 4628, 7438, 1820, 2, 2717, 11, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The- Software.aged Design Services</s> Development in</s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The- Software.aged Design Services Development in'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 11143, 4, 4628, 7438, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'The- Software.aged Design Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.62it/s, loss=5.4397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Avg Validation Loss: 7.1406\n",
      "  Avg ROUGE-1: 9.0271, ROUGE-2: 1.5711, ROUGE-L: 8.8182\n",
      "  Avg BLEU: 2.2735\n",
      "\n",
      "--- Epoch 15/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.95it/s, loss=4.7450, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Avg Train Loss: 4.7289, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 37.36it/s, loss=6.4546]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 15 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 7, 7, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The to to for</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The to to for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 7, 7, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The to to for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 15 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 12, 154, 1260, 359, 154, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Web-ing Company &ing</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Web-ing Company &ing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 12, 154, 1260, 359, 154]\n",
      "  FINAL Decoded Prediction (for metric): 'Web-ing Company &ing'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 15 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 12, 6282, 4, 4628, 3779, 1820, 2, 6698, 11, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Web- Digital.aged IT Services</s> Mobile in</s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Web- Digital.aged IT Services Mobile in'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 12, 6282, 4, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Web- Digital.aged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 37.91it/s, loss=5.3698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Avg Validation Loss: 7.0904\n",
      "  Avg ROUGE-1: 9.8447, ROUGE-2: 1.9642, ROUGE-L: 9.5174\n",
      "  Avg BLEU: 2.4527\n",
      "\n",
      "--- Epoch 16/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.90it/s, loss=4.8550, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Avg Train Loss: 4.5692, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 37.69it/s, loss=6.4430]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 16 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 7, 7, 7874, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The to to Solutions</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The to to Solutions'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 7, 7, 7874]\n",
      "  FINAL Decoded Prediction (for metric): 'The to to Solutions'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 16 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 1260, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing Company &</s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing Company &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 1260, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing Company &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 16 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [250, 12, 11143, 12, 4628, 3779, 1820, 2, 2090, 7874, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'A- Software-aged IT Services</s> Business Solutions</s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'A- Software-aged IT Services Business Solutions'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [250, 12, 11143, 12, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'A- Software-aged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.10it/s, loss=5.2740]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Avg Validation Loss: 7.1790\n",
      "  Avg ROUGE-1: 9.5192, ROUGE-2: 1.7801, ROUGE-L: 9.3129\n",
      "  Avg BLEU: 2.4312\n",
      "\n",
      "--- Epoch 17/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.96it/s, loss=4.6434, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Avg Train Loss: 4.4021, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 37.60it/s, loss=6.6024]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 17 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 7, 7, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The to to and</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The to to and'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 7, 7, 8]\n",
      "  FINAL Decoded Prediction (for metric): 'The to to and'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 17 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 12, 154, 359, 359, 154, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The-ing & &ing</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The-ing & &ing'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 12, 154, 359, 359, 154]\n",
      "  FINAL Decoded Prediction (for metric): 'The-ing & &ing'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 17 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 1851, 11143, 4, 4628, 3779, 1820, 2, 2090, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Theain Software.aged IT Services</s> Business</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Theain Software.aged IT Services Business'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 1851, 11143, 4, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Theain Software.aged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.24it/s, loss=5.3514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Avg Validation Loss: 7.3394\n",
      "  Avg ROUGE-1: 9.7942, ROUGE-2: 1.7792, ROUGE-L: 9.4230\n",
      "  Avg BLEU: 2.5135\n",
      "\n",
      "--- Epoch 18/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.96it/s, loss=4.2437, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Avg Train Loss: 4.2317, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.20it/s, loss=6.5205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 18 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 7, 13, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The to for for</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The to for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 7, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The to for for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 18 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [19183, 12, 154, 359, 359, 359, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Best-ing & & &</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Best-ing & & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [19183, 12, 154, 359, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'Best-ing & & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 18 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [2068, 12, 11143, 4, 4628, 3779, 1820, 2, 2090, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'IT- Software.aged IT Services</s> Business</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'IT- Software.aged IT Services Business'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [2068, 12, 11143, 4, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'IT- Software.aged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 37.94it/s, loss=5.1755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Avg Validation Loss: 7.2262\n",
      "  Avg ROUGE-1: 9.9300, ROUGE-2: 1.9164, ROUGE-L: 9.5305\n",
      "  Avg BLEU: 2.5762\n",
      "\n",
      "--- Epoch 19/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.96it/s, loss=4.1838, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Avg Train Loss: 4.0761, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.43it/s, loss=6.4738]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 19 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 7, 7, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The to to for</s></s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The to to for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 7, 7, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The to to for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 19 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 3777, 154, 359, 359, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The Technologying & &</s></s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The Technologying & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 3777, 154, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'The Technologying & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 19 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [104, 1851, 104, 13129, 4628, 3779, 1820, 2, 6698, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'SainS Personalaged IT Services</s> Mobile</s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'SainS Personalaged IT Services Mobile'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [104, 1851, 104, 13129, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'SainS Personalaged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.17it/s, loss=5.2371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Avg Validation Loss: 7.1845\n",
      "  Avg ROUGE-1: 9.7997, ROUGE-2: 1.9737, ROUGE-L: 9.5088\n",
      "  Avg BLEU: 2.6386\n",
      "\n",
      "--- Epoch 20/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [T]: 100%|██████████| 134/134 [00:11<00:00, 11.96it/s, loss=3.9000, lr=0.0001000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Avg Train Loss: 3.9109, LR: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [V]:  12%|█▏        | 4/34 [00:00<00:00, 38.85it/s, loss=6.5054]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 20 Validation Sample 0 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [133, 7, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'The to for for for for for for for for for for for for'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'The to for for for for for for for for for for for for'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [133, 7, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]\n",
      "  FINAL Decoded Prediction (for metric): 'The to for for for for for for for for for for for for'\n",
      "  Raw Reference IDs: [32998, 52, 1119, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Together we build</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [32998, 52, 1119]\n",
      "  FINAL Decoded Reference (for metric): 'Together we build'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 20 Validation Sample 1 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [250, 575, 154, 1260, 359, 359, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'A careing Company & &</s></s></s></s></s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'A careing Company & &'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [250, 575, 154, 1260, 359, 359]\n",
      "  FINAL Decoded Prediction (for metric): 'A careing Company & &'\n",
      "  Raw Reference IDs: [35935, 10326, 154, 25503, 10247, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'Better Cleaning Guaranteed</s><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [35935, 10326, 154, 25503, 10247]\n",
      "  FINAL Decoded Reference (for metric): 'Better Cleaning Guaranteed'\n",
      "--- End Sample ---\n",
      "\n",
      "--- Epoch 20 Validation Sample 2 ---\n",
      "Pad Token ID: 1, EOS Token ID: 2\n",
      "  Raw Predicted IDs: [27521, 1851, 6282, 4, 4628, 3779, 1820, 2, 2090, 7874, 2, 2, 2, 2]\n",
      "  Decoded Raw Predicted (skip_special_tokens=False): 'Webain Digital.aged IT Services</s> Business Solutions</s></s></s></s>'\n",
      "  Decoded Raw Predicted (skip_special_tokens=True): 'Webain Digital.aged IT Services Business Solutions'\n",
      "  Processed Predicted IDs (for final decode, after EOS cut): [27521, 1851, 6282, 4, 4628, 3779, 1820]\n",
      "  FINAL Decoded Prediction (for metric): 'Webain Digital.aged IT Services'\n",
      "  Raw Reference IDs: [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575, 2, 1, 1, 1]\n",
      "  Decoded Raw Reference (skip_special_tokens=False): 'St. Louis Managed IT Services and Print Hardware</s><pad><pad><pad>'\n",
      "  Processed Reference IDs (for final decode, after PAD/EOS filter): [5320, 4, 3217, 1554, 4628, 3779, 1820, 8, 6883, 29575]\n",
      "  FINAL Decoded Reference (for metric): 'St. Louis Managed IT Services and Print Hardware'\n",
      "--- End Sample ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [V]: 100%|██████████| 34/34 [00:00<00:00, 38.82it/s, loss=5.2467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Avg Validation Loss: 7.1819\n",
      "  Avg ROUGE-1: 10.4006, ROUGE-2: 1.9492, ROUGE-L: 10.0961\n",
      "  Avg BLEU: 2.6534\n",
      "\n",
      "Training finished.\n",
      "Final model weights saved to final_ipynb_transformer.pt (if not overwritten by better validation scores).\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epochs=EPOCHS_TRAIN, model_save_path=\"final_ipynb_transformer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (source_embedding): Embedding(50265, 512)\n",
       "  (target_embedding): Embedding(50265, 512)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=50265, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"final_ipynb_transformer.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape        Output Shape       Param #\n",
       "=======================================================================================================================\n",
       "Transformer (Transformer)                                         [64, 128]          [64, 127, 50265]   --\n",
       "├─Embedding (source_embedding)                                    [64, 128]          [64, 128, 512]     25,735,680\n",
       "├─Embedding (target_embedding)                                    [64, 127]          [64, 127, 512]     25,735,680\n",
       "├─PositionalEncoding (pos_encoder)                                [64, 128, 512]     [64, 128, 512]     --\n",
       "│    └─Dropout (dropout)                                          [64, 128, 512]     [64, 128, 512]     --\n",
       "├─PositionalEncoding (pos_encoder)                                [64, 127, 512]     [64, 127, 512]     --\n",
       "│    └─Dropout (dropout)                                          [64, 127, 512]     [64, 127, 512]     --\n",
       "├─Transformer (transformer)                                       [64, 128, 512]     [64, 127, 512]     --\n",
       "│    └─TransformerEncoder (encoder)                               [64, 128, 512]     [64, 128, 512]     --\n",
       "│    │    └─ModuleList (layers)                                   --                 --                 18,914,304\n",
       "│    │    └─LayerNorm (norm)                                      [64, 128, 512]     [64, 128, 512]     1,024\n",
       "│    └─TransformerDecoder (decoder)                               [64, 127, 512]     [64, 127, 512]     --\n",
       "│    │    └─ModuleList (layers)                                   --                 --                 25,224,192\n",
       "│    │    └─LayerNorm (norm)                                      [64, 127, 512]     [64, 127, 512]     1,024\n",
       "├─Linear (fc_out)                                                 [64, 127, 512]     [64, 127, 50265]   25,785,945\n",
       "=======================================================================================================================\n",
       "Total params: 121,397,849\n",
       "Trainable params: 121,397,849\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 6.56\n",
       "=======================================================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 6409.44\n",
       "Params size (MB): 409.95\n",
       "Estimated Total Size (MB): 6819.52\n",
       "======================================================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.randint(0, 50265, (64, 128)).to(device)\n",
    "tgt = torch.randint(0, 50265, (64, 127)).to(device)\n",
    "\n",
    "summary(model, \n",
    "        input_data=(src, tgt),\n",
    "        depth=3,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        row_settings=[\"var_names\"],\n",
    "        col_width=18\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layer (name)                        | Output Shape      | # Params     |\n",
    "|------------------------------------|-------------------|--------------|\n",
    "| Embedding (source_embedding)       | [64, 128, 512]    | 25,735,680   |\n",
    "| Embedding (target_embedding)       | [64, 127, 512]    | 25,735,680   |\n",
    "| PositionalEncoding (src)           | [64, 128, 512]    | 0            |\n",
    "| PositionalEncoding (tgt)           | [64, 127, 512]    | 0            |\n",
    "| TransformerEncoder (6 layers)      | [64, 128, 512]    | 18,914,304   |\n",
    "| TransformerDecoder (6 layers)      | [64, 127, 512]    | 25,224,192   |\n",
    "| Linear (fc_out)                    | [64, 127, 50265]  | 25,785,945   |\n",
    "| **Total**                          | —                 | **121,397,849** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 121,397,849\n",
      "Trainable parameters: 121,397,849\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slogan(description: str,\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    max_len: int,\n",
    "                    bos_id: int,\n",
    "                    eos_id: int,\n",
    "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Greedy‑decode one slogan from a single description string.\n",
    "    \"\"\"\n",
    "    enc = tokenizer(description,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=max_len)\n",
    "    src_ids   = enc[\"input_ids\"].to(device)\n",
    "    src_mask  = enc[\"attention_mask\"].to(device)\n",
    "    src_key_padding_mask = (src_mask == 0)\n",
    "\n",
    "    dec_ids = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n",
    "    generated = [bos_id]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            logits = model(src=src_ids,\n",
    "                           tgt=dec_ids,\n",
    "                           src_padding_mask=src_key_padding_mask,\n",
    "                           memory_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "            next_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "            dec_ids = torch.cat([dec_ids, next_id.unsqueeze(0)], dim=1)\n",
    "            generated.append(next_id.item())\n",
    "\n",
    "            if next_id.item() == eos_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = ['Easily deliver personalized activities that enrich the lives of residents in older adult communities. Save time and increase satisfaction.',\n",
    "'Powerful lead generation software that converts abandoning visitors into subscribers with our dynamic marketing tools and Exit Intent® technology.',\n",
    "\"Twine matches companies to the best digital and creative freelancers from a network of over 260,000. It's free to post a job and you only pay when you hire.\",\n",
    "\"Looking for fresh web design & development? Need new marketing materials or a smart campaign to drive business? How about a video or updated photos? Let's talk and tell the world your story.\",\n",
    "# --- test-curated.csv\n",
    "'Our expert team of Analytical Chemists provide eLiquid analysis & manufacturing services, ensuring full regulatory compliance for the e-cigarette market.',\n",
    "'From placing entire software engineering teams to integrating easily into your current team, we offer bespoke placements of the very best engineers.',\n",
    "'Turning ideas into visual content since 1999. Content Creation Studio in Ghent. Branded content - corporate video - visuals for events - 360 video',\n",
    "'World market leader for robotic vision systems, inline measurement technology & inspection technology. We are your partner at over 25 locations worldwide.',\n",
    "# --- other examples\n",
    "'People and projects for sustainable change. Experts in sustainability recruitment, we recruit exceptional people into roles working on sustainability projects or in ethical and responsible organisations.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_set.iloc[3][\"desc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. The Best of the best of the UK\n",
      "02. The Best of the best of the best of the UK\n",
      "03. The Best of the best of the UK\n",
      "04. The Best Digital Marketing Agency in India\n",
      "05. The Best's Best Digital Marketing\n",
      "06. The Best of the best of the UK\n",
      "07. The Best's leading UK of the UK\n",
      "08. The Best's Best of Sale\n",
      "09. The Best's Best of Sale\n"
     ]
    }
   ],
   "source": [
    "for idx, desc in enumerate(examples, 1):\n",
    "    slogan = generate_slogan(desc,\n",
    "                             model=model,\n",
    "                             tokenizer=tokenizer,\n",
    "                             max_len=MAX_DESC_LEN,\n",
    "                             bos_id=BOS_TOKEN_ID,\n",
    "                             eos_id=EOS_TOKEN_ID,\n",
    "                             device=device)\n",
    "    print(f\"{idx:02d}. {slogan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(example, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_DESC_LEN)\n",
    "source_tensor = encoded_input['input_ids'].to(device)\n",
    "source_attention_mask = encoded_input['attention_mask'].to(device)\n",
    "\n",
    "src_key_padding_mask = (source_attention_mask == 0)\n",
    "\n",
    "target_tensor_input = torch.tensor([[BOS_TOKEN_ID]], dtype=torch.long, device=device)\n",
    "generated_ids = [BOS_TOKEN_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for _ in range(MAX_DESC_LEN):\n",
    "        tgt_mask = model._generate_square_subsequent_mask(target_tensor_input.size(1), device)\n",
    "        output_logits = model(src=source_tensor, \n",
    "                              tgt=target_tensor_input, \n",
    "                              src_padding_mask=src_key_padding_mask, \n",
    "                              memory_key_padding_mask=src_key_padding_mask\n",
    "                            #   tgt_mask=tgt_mask\n",
    "                              )\n",
    "        next_token_logits = output_logits[:, -1, :]\n",
    "        predicted_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "        target_tensor_input = torch.cat((target_tensor_input, predicted_token_id.unsqueeze(1)), dim=1)\n",
    "        generated_ids.append(predicted_token_id.item())\n",
    "\n",
    "        if predicted_token_id.item() == EOS_TOKEN_ID:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "slogan = tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Best Best's Most Trusted in the UK\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slogan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
